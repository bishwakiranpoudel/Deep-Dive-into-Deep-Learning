# Deep Dive into Deep Learning

## Overview

**Deep Dive into Deep Learning** is a comprehensive project where I implemented 10 significant deep learning architectures from scratch. This project is designed to deepen understanding and demonstrate practical knowledge of state-of-the-art deep learning models across various domains, including computer vision, natural language processing, and generative modeling.

## Implemented Models

### 1. **AlexNet**

- **Domain**: Computer Vision
- **Highlights**: Introduced convolutional neural networks (CNNs) to large-scale image classification, achieving groundbreaking results in the ImageNet Challenge.

### 2. **Attention Is All You Need**

- **Domain**: NLP
- **Highlights**: Foundation of the transformer architecture, focusing entirely on self-attention mechanisms for sequence processing.

### 3. **BERT (Bidirectional Encoder Representations from Transformers)**

- **Domain**: NLP
- **Highlights**: Pretrained transformer-based model for tasks like text classification, question answering, and named entity recognition.

### 4. **GAN (Generative Adversarial Networks)**

- **Domain**: Generative Modeling
- **Highlights**: Utilized adversarial training between generator and discriminator networks for realistic image synthesis.

### 5. **GPT (Generative Pretrained Transformer)**

- **Domain**: NLP
- **Highlights**: Autoregressive transformer model designed for text generation, with applications in dialogue systems and creative writing.

### 6. **LLAMA (Large Language Model Meta AI)**

- **Domain**: NLP
- **Highlights**: Efficient and scalable transformer-based large language model optimized for research and practical applications.

### 7. **Neural Style Transfer**

- **Domain**: Computer Vision
- **Highlights**: Implemented the artistic rendering of content images in the style of a chosen reference image using CNNs.

### 8. **ResNet (Residual Networks)**

- **Domain**: Computer Vision
- **Highlights**: Introduced skip connections to address vanishing gradients, enabling the training of very deep networks.

### 9. **VGG (Visual Geometry Group Network)**

- **Domain**: Computer Vision
- **Highlights**: Demonstrated the impact of small convolutional filters for image classification.

### 10. **YOLO (You Only Look Once)**

- **Domain**: Object Detection
- **Highlights**: Real-time object detection model known for its speed and efficiency.

### 11. **LLM Quantization (Large Language Model Quantization)**

- **Domain**: Natural Language Processing
- **Highlights**: Focuses on reducing the memory footprint and computational requirements of large language models by representing weights and activations in lower-precision formats (e.g., INT8, INT4) without significant loss in model accuracy. This technique enables faster inference and deployment on edge devices and resource-constrained environments.

## Key Features

- **From-Scratch Implementations**: Each model is implemented from scratch using Python and deep learning frameworks such as TensorFlow or PyTorch.
- **Jupyter Notebooks**: Each model is available as an individual `.ipynb` notebook for easy execution and visualization.
- **Cross-Domain Expertise**: Covers diverse areas of deep learning, showcasing versatility and depth.

## Usage

Each model is available as a separate Jupyter Notebook in the root directory. To run a specific model:

1. Open the desired notebook. For example:
   ```bash
   jupyter notebook AlexNet.ipynb
   ```
2. Execute the cells sequentially to train or test the model and visualize the results.

## Directory Structure

```
.
├── alexnet.ipynb
├── attention_is_all_you_need.ipynb
├── bert.ipynb
├── gan.ipynb
├── gpt.ipynb
├── llama.ipynb
├── neural_style_transfer.ipynb
├── resnet.ipynb
├── vgg.ipynb
├── yolo.ipynb
├── outputs

├── README.md
```

**Deep Dive into Deep Learning** © 2025. All rights reserved.

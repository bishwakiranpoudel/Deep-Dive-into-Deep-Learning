{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "flrUXVHuxBNo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from transformers import AutoTokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RotaryEmbedding(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.shape[1]\n",
        "        t = torch.arange(seq_len, device=x.device).float()\n",
        "        sinusoid = torch.einsum('n , d -> n d', t, self.inv_freq)\n",
        "        cos, sin = sinusoid.cos(), sinusoid.sin()\n",
        "        return torch.cat([cos, sin], dim=-1)\n"
      ],
      "metadata": {
        "id": "g-3Mpaxwxng3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(query.size(-1))\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask.unsqueeze(1).unsqueeze(2) == 0, -1e9)\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        return torch.matmul(attn, value)\n"
      ],
      "metadata": {
        "id": "eeB4ppoOxonX"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.qkv_proj = nn.Linear(embed_dim, 3 * embed_dim)\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, embed_dim = x.size()\n",
        "        qkv = self.qkv_proj(x).view(batch_size, seq_len, self.num_heads, 3 * self.head_dim)\n",
        "        query, key, value = torch.chunk(qkv, 3, dim=-1)\n",
        "\n",
        "        # Transpose for multi-head attention\n",
        "        query = query.permute(0, 2, 1, 3)\n",
        "        key = key.permute(0, 2, 1, 3)\n",
        "        value = value.permute(0, 2, 1, 3)\n",
        "\n",
        "        # Scaled Dot-Product Attention\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask.unsqueeze(1).unsqueeze(2) == 0, -1e9)\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "\n",
        "        # Multiply attention weights with value\n",
        "        attn_output = torch.matmul(attn, value)\n",
        "        attn_output = attn_output.permute(0, 2, 1, 3).contiguous()\n",
        "        attn_output = attn_output.view(batch_size, seq_len, embed_dim)\n",
        "        return self.out_proj(attn_output)\n"
      ],
      "metadata": {
        "id": "FXZO3jQLxpwk"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embed_dim, expansion_factor=4):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(embed_dim, expansion_factor * embed_dim)\n",
        "        self.fc2 = nn.Linear(expansion_factor * embed_dim, embed_dim)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.dropout(x)\n",
        "        return self.fc2(x)\n"
      ],
      "metadata": {
        "id": "QPLKfA4MxrLB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(embed_dim)\n",
        "        self.mha = MultiHeadAttention(embed_dim, num_heads)\n",
        "        self.ln2 = nn.LayerNorm(embed_dim)\n",
        "        self.ffn = FeedForward(embed_dim)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = x + self.mha(self.ln1(x), mask)\n",
        "        x = x + self.ffn(self.ln2(x))\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "LO9Acv_jxsct"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LLaMA(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.layers = nn.ModuleList([TransformerBlock(embed_dim, num_heads) for _ in range(num_layers)])\n",
        "        self.ln = nn.LayerNorm(embed_dim)\n",
        "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = self.embedding(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        x = self.ln(x)\n",
        "        return self.fc(x)\n"
      ],
      "metadata": {
        "id": "nVaT3xY-xv67"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "vocab_size = 30000\n",
        "embed_dim = 512\n",
        "num_heads = 8\n",
        "num_layers = 6\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Model\n",
        "model = LLaMA(vocab_size, embed_dim, num_heads, num_layers)\n",
        "\n",
        "# Example input\n",
        "input_text = [\"Deep learning is amazing\", \"Transformers are powerful\"]\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).input_ids\n",
        "\n",
        "\n",
        "# Training setup\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
        "\n",
        "# Forward pass\n",
        "output = model(inputs)\n",
        "loss = criterion(output.view(-1, vocab_size), inputs.view(-1))\n",
        "\n",
        "loss.backward()\n",
        "optimizer.step()\n"
      ],
      "metadata": {
        "id": "BRdtETLCxxg0"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}
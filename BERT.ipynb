{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BERT\n",
        "### Key Innovations\n",
        "- Bidirectional Training: Unlike previous models like GPT that were unidirectional (left-to-right or right-to-left), BERT employs a bidirectional training approach using transformers.\n",
        "\n",
        "- Masked Language Modeling (MLM): BERT uses MLM to pre-train by randomly masking tokens and predicting them based on context.\n",
        "\n",
        "- Next Sentence Prediction (NSP): BERT learns sentence relationships, enabling tasks like Question Answering (QA) and Natural Language Inference (NLI).\n",
        "\n",
        "- Transfer Learning: Pre-trained on large corpora (e.g., Wikipedia, BookCorpus) and fine-tuned for specific tasks."
      ],
      "metadata": {
        "id": "V-FB5ZDAvs8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n"
      ],
      "metadata": {
        "id": "g3oxlpSPv7_8"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "SUhK0KHSwDbq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class JointEmbedding(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, size):\n",
        "        super(JointEmbedding, self).__init__()\n",
        "\n",
        "        self.size = size\n",
        "\n",
        "        self.token_emb = nn.Embedding(vocab_size, size)\n",
        "        self.segment_emb = nn.Embedding(vocab_size, size)\n",
        "\n",
        "        self.norm = nn.LayerNorm(size)\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        sentence_size = input_tensor.size(-1)\n",
        "        pos_tensor = self.attention_position(self.size, input_tensor)\n",
        "\n",
        "        segment_tensor = torch.zeros_like(input_tensor).to(device)\n",
        "        segment_tensor[:, sentence_size // 2 + 1:] = 1\n",
        "\n",
        "        output = self.token_emb(input_tensor) + self.segment_emb(segment_tensor) + pos_tensor\n",
        "        return self.norm(output)\n",
        "\n",
        "    def attention_position(self, dim, input_tensor):\n",
        "        batch_size = input_tensor.size(0)\n",
        "        sentence_size = input_tensor.size(-1)\n",
        "\n",
        "        pos = torch.arange(sentence_size, dtype=torch.long).to(device)\n",
        "        d = torch.arange(dim, dtype=torch.long).to(device)\n",
        "        d = (2 * d / dim)\n",
        "\n",
        "        pos = pos.unsqueeze(1)\n",
        "        pos = pos / (1e4 ** d)\n",
        "\n",
        "        pos[:, ::2] = torch.sin(pos[:, ::2])\n",
        "        pos[:, 1::2] = torch.cos(pos[:, 1::2])\n",
        "\n",
        "        return pos.expand(batch_size, *pos.size())\n",
        "\n",
        "    def numeric_position(self, dim, input_tensor):\n",
        "        pos_tensor = torch.arange(dim, dtype=torch.long).to(device)\n",
        "        return pos_tensor.expand_as(input_tensor)"
      ],
      "metadata": {
        "id": "BTc0xmhuwE4k"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionHead(nn.Module):\n",
        "\n",
        "    def __init__(self, dim_inp, dim_out):\n",
        "        super(AttentionHead, self).__init__()\n",
        "\n",
        "        self.dim_inp = dim_inp\n",
        "\n",
        "        self.q = nn.Linear(dim_inp, dim_out)\n",
        "        self.k = nn.Linear(dim_inp, dim_out)\n",
        "        self.v = nn.Linear(dim_inp, dim_out)\n",
        "\n",
        "    def forward(self, input_tensor: torch.Tensor, attention_mask: torch.Tensor = None):\n",
        "        query, key, value = self.q(input_tensor), self.k(input_tensor), self.v(input_tensor)\n",
        "\n",
        "        scale = query.size(1) ** 0.5\n",
        "        scores = torch.bmm(query, key.transpose(1, 2)) / scale\n",
        "\n",
        "        scores = scores.masked_fill_(attention_mask, -1e9)\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        context = torch.bmm(attn, value)\n",
        "\n",
        "        return context"
      ],
      "metadata": {
        "id": "KarIZ5IewT_J"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, num_heads, dim_inp, dim_out):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "        self.heads = nn.ModuleList([\n",
        "            AttentionHead(dim_inp, dim_out) for _ in range(num_heads)\n",
        "        ])\n",
        "        self.linear = nn.Linear(dim_out * num_heads, dim_inp)\n",
        "        self.norm = nn.LayerNorm(dim_inp)\n",
        "\n",
        "    def forward(self, input_tensor: torch.Tensor, attention_mask: torch.Tensor):\n",
        "        s = [head(input_tensor, attention_mask) for head in self.heads]\n",
        "        scores = torch.cat(s, dim=-1)\n",
        "        scores = self.linear(scores)\n",
        "        return self.norm(scores)"
      ],
      "metadata": {
        "id": "7Mz3mqKKwXYA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, dim_inp, dim_out, attention_heads=4, dropout=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.attention = MultiHeadAttention(attention_heads, dim_inp, dim_out)  # batch_size x sentence size x dim_inp\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(dim_inp, dim_out),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(dim_out, dim_inp),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        self.norm = nn.LayerNorm(dim_inp)\n",
        "\n",
        "    def forward(self, input_tensor: torch.Tensor, attention_mask: torch.Tensor):\n",
        "        context = self.attention(input_tensor, attention_mask)\n",
        "        res = self.feed_forward(context)\n",
        "        return self.norm(res)"
      ],
      "metadata": {
        "id": "uBGMxbW8wmuF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BERT(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, dim_inp, dim_out, attention_heads=4):\n",
        "        super(BERT, self).__init__()\n",
        "\n",
        "        self.embedding = JointEmbedding(vocab_size, dim_inp)\n",
        "        self.encoder = Encoder(dim_inp, dim_out, attention_heads)\n",
        "\n",
        "        self.token_prediction_layer = nn.Linear(dim_inp, vocab_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=-1)\n",
        "        self.classification_layer = nn.Linear(dim_inp, 2)\n",
        "\n",
        "    def forward(self, input_tensor: torch.Tensor, attention_mask: torch.Tensor):\n",
        "        embedded = self.embedding(input_tensor)\n",
        "        encoded = self.encoder(embedded, attention_mask)\n",
        "\n",
        "        token_predictions = self.token_prediction_layer(encoded)\n",
        "\n",
        "        first_word = encoded[:, 0, :]\n",
        "        return self.softmax(token_predictions), self.classification_layer(first_word)"
      ],
      "metadata": {
        "id": "L278C1wCwva2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I-vDyTmGwySx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Generative Pretrained Transformer (GPT)\n",
        "\n",
        "### Key Innovations\n",
        "- transformer Decoder: GPT uses the decoder portion of the transformer, focusing on autoregressive text generation ( predicting next token )\n",
        "\n",
        "- Unidirectional Context: GPT is trained to predict the next word based on the left context\n"
      ],
      "metadata": {
        "id": "jKvAquE5qGPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "Oprujdevqm-q"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# making config class for hyperparameters\n",
        "\n",
        "class GPTConfig:\n",
        "    attn_dropout = 0.1\n",
        "    embed_dropout = 0.1\n",
        "    ff_dropout = 0.1\n",
        "\n",
        "    def __init__(\n",
        "        self, vocab_size, max_len, **kwargs\n",
        "    ):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_len = max_len\n",
        "        for key, value in kwargs.items():\n",
        "            setattr(self, key, value)\n",
        "\n",
        "class GPT1Config(GPTConfig):\n",
        "    num_heads = 12\n",
        "    num_blocks = 12\n",
        "    embed_dim = 768"
      ],
      "metadata": {
        "id": "bPfRmWQ_qq-H"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        embed_dim = config.embed_dim\n",
        "        self.ln1 = nn.LayerNorm(embed_dim)\n",
        "        self.ln2 = nn.LayerNorm(embed_dim)\n",
        "        self.attn = MultiheadAttention(config)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_dim * 4),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(embed_dim * 4, embed_dim),\n",
        "            nn.Dropout(config.ff_dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.ff(self.ln2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "Z0g5HNxOrSOU"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        embed_dim = config.embed_dim\n",
        "        self.num_heads = config.num_heads\n",
        "        assert embed_dim % self.num_heads == 0, \"invalid heads and embedding dimension configuration\"\n",
        "        self.key = nn.Linear(embed_dim, embed_dim)\n",
        "        self.value = nn.Linear(embed_dim, embed_dim)\n",
        "        self.query = nn.Linear(embed_dim, embed_dim)\n",
        "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.attn_dropout = nn.Dropout(config.attn_dropout)\n",
        "        self.proj_dropout = nn.Dropout(config.ff_dropout)\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.tril(torch.ones(config.max_len, config.max_len))\n",
        "            .unsqueeze(0).unsqueeze(0)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        seq_len = x.size(1)\n",
        "\n",
        "        k_t = self.key(x).reshape(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n",
        "        v = self.value(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "        q = self.query(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "\n",
        "\n",
        "        attn = torch.matmul(q, k_t) / math.sqrt(q.size(-1))\n",
        "\n",
        "        mask = self.mask[:, :, :seq_len, :seq_len]\n",
        "        attn = attn.masked_fill(mask == 0, float(\"-inf\"))\n",
        "        attn = self.attn_dropout(attn)\n",
        "\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "        y = torch.matmul(attn, v)\n",
        "\n",
        "        y = y.transpose(1, 2)\n",
        "\n",
        "        y = y.reshape(batch_size, seq_len, -1)\n",
        "\n",
        "        y = self.proj_dropout(self.proj(y))\n",
        "        return y"
      ],
      "metadata": {
        "id": "e_qfTbAEswI1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        embed_dim = config.embed_dim\n",
        "        self.max_len = config.max_len\n",
        "        self.tok_embed = nn.Embedding(\n",
        "            config.vocab_size, embed_dim\n",
        "        )\n",
        "        self.pos_embed = nn.Parameter(\n",
        "            torch.zeros(1, config.max_len, embed_dim)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(config.embed_dropout)\n",
        "        self.blocks = nn.Sequential(\n",
        "            *[Block(config) for _ in range(config.num_blocks)]\n",
        "        )\n",
        "        self.ln = nn.LayerNorm(embed_dim)\n",
        "        self.fc = nn.Linear(embed_dim, config.vocab_size)\n",
        "\n",
        "    def forward(self, x, target=None):\n",
        "        # batch_size = x.size(0)\n",
        "        seq_len = x.size(1)\n",
        "        assert seq_len <= self.max_len, \"sequence longer than model capacity\"\n",
        "\n",
        "        tok_embedding = self.tok_embed(x)\n",
        "        # tok_embedding.shape == (batch_size, seq_len, embed_dim)\n",
        "        pos_embedding = self.pos_embed[:, :seq_len, :]\n",
        "        # pos_embedding.shape == (1, seq_len, embed_dim)\n",
        "        x = self.dropout(tok_embedding + pos_embedding)\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln(x)\n",
        "        x = self.fc(x)\n",
        "        # x.shape == (batch_size, seq_len, vocab_size)\n",
        "        return x"
      ],
      "metadata": {
        "id": "J91CdwaxrIXC"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 10\n",
        "max_len = 12\n",
        "\n",
        "config = GPT1Config(vocab_size, max_len)\n",
        "gpt = GPT(config)"
      ],
      "metadata": {
        "id": "3ZTe7Zd2t5_A"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_weights(module):\n",
        "    if isinstance(module, torch.nn.Linear):\n",
        "        torch.nn.init.xavier_uniform_(module.weight)\n",
        "        if module.bias is not None:\n",
        "            torch.nn.init.zeros_(module.bias)\n",
        "    elif isinstance(module, torch.nn.Embedding):\n",
        "        torch.nn.init.normal_(module.weight, mean=0, std=1)\n",
        "\n",
        "# Apply initialization\n",
        "gpt.apply(initialize_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1UchGhautt_",
        "outputId": "24f15998-1b99-4f27-9feb-659a0e53ff27"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT(\n",
              "  (tok_embed): Embedding(10, 768)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (blocks): Sequential(\n",
              "    (0): Block(\n",
              "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): MultiheadAttention(\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "        (proj_dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): Sequential(\n",
              "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (1): Block(\n",
              "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): MultiheadAttention(\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "        (proj_dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): Sequential(\n",
              "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (2): Block(\n",
              "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): MultiheadAttention(\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "        (proj_dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): Sequential(\n",
              "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (3): Block(\n",
              "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): MultiheadAttention(\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "        (proj_dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): Sequential(\n",
              "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (4): Block(\n",
              "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): MultiheadAttention(\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "        (proj_dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): Sequential(\n",
              "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (5): Block(\n",
              "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): MultiheadAttention(\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "        (proj_dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): Sequential(\n",
              "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (6): Block(\n",
              "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): MultiheadAttention(\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "        (proj_dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): Sequential(\n",
              "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (7): Block(\n",
              "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): MultiheadAttention(\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "        (proj_dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): Sequential(\n",
              "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (8): Block(\n",
              "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): MultiheadAttention(\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "        (proj_dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): Sequential(\n",
              "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (9): Block(\n",
              "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): MultiheadAttention(\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "        (proj_dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): Sequential(\n",
              "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (10): Block(\n",
              "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): MultiheadAttention(\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "        (proj_dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): Sequential(\n",
              "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (11): Block(\n",
              "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): MultiheadAttention(\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "        (proj_dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): Sequential(\n",
              "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  (fc): Linear(in_features=768, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(gpt.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "input_tokens = torch.randint(0, vocab_size, (32, max_len))\n",
        "target_tokens = torch.randint(0, vocab_size, (32, max_len))\n",
        "\n",
        "for epoch in range(10):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = gpt(input_tokens)\n",
        "    loss = criterion(outputs.view(-1, vocab_size), target_tokens.view(-1))\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(gpt.parameters(), max_norm=1.0)  # Gradient clipping\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch}, Loss: {loss.item()}\")"
      ],
      "metadata": {
        "id": "iOQ5FFUUuT5L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
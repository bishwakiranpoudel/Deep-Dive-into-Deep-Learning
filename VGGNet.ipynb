{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# VGGNet\n",
        "## Key Elements\n",
        "#### Objective:\n",
        "Explore the effect of convolutiona network depth on image recognition accuracy.\n",
        "\n",
        "#### Innovations:\n",
        "- Use of small (3x3) filters consistently throughout the network insted of larger ones like 11x11 or 5x5\n",
        "- Doubling the number of filters after every pooling operation, enabling a deeper network to capture more complex features.\n",
        "- Simplicity in design; all convolution layers have the same kernal size and stride.\n",
        "\n",
        "## Architecture\n",
        "- Input: 224X224 RGB images.\n",
        "- Layers: A series of 3x3 convolutiona layers, followed by a pooling layer.\n",
        "- Blocks: Each block contains multiple convolution layers, followed by a pooling layer.\n",
        "- Fully Connected: Three dense layers at the end with ReLU and Dropout.\n",
        "- Output: Softmax activation for classification.\n",
        "\n",
        "I will be implementing VGG-16."
      ],
      "metadata": {
        "id": "5bFdnCaDR2cL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing all the necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n"
      ],
      "metadata": {
        "id": "2Tb3EQptS8Cq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG16(nn.Module):\n",
        "\n",
        "  def __init__(self, num_classes=10):\n",
        "    super(VGG16, self).__init__()\n",
        "    self.features = nn.Sequential(\n",
        "        #Block 1\n",
        "        nn.Conv2d(3,64, kernel_size=3, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(64,64, kernel_size=3, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        #Block 2\n",
        "        nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "        # Block 3\n",
        "        nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "        # Block 4\n",
        "        nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "        # Block 5\n",
        "        nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "    )\n",
        "\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Linear(512 * 7 * 7, 4096),\n",
        "        nn.ReLU(inplace = True),\n",
        "        nn.Dropout(),\n",
        "        nn.Linear(4096, 4096),\n",
        "        nn.ReLU(inplace = True),\n",
        "        nn.Dropout(),\n",
        "        nn.Linear(4096, num_classes),\n",
        "     )\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.features(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = self.classifier(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "iEjRJ9HzTIRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function\n",
        "def train_model(model, dataloader, criterion, optimizer, epochs=3, device=\"cuda\"):\n",
        "    model.to(device)\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(dataloader):.4f}\")\n"
      ],
      "metadata": {
        "id": "lIG2M_eGVHks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data preparation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# CIFAR-10 dataset for simplicity\n",
        "train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "# Instantiate model, loss function, and optimizer\n",
        "model = VGG16(num_classes=10)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "train_model(model, train_loader, criterion, optimizer, epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JO6zDbcAVMw5",
        "outputId": "fd544d70-2f4f-48e0-f3b1-0f43b667cbf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 48.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Epoch 1/5, Loss: 2.3042\n",
            "Epoch 2/5, Loss: 2.3028\n",
            "Epoch 3/5, Loss: 2.3028\n",
            "Epoch 4/5, Loss: 2.3027\n",
            "Epoch 5/5, Loss: 2.3027\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer (Attention is all you need)\n",
        "\n",
        "## Key Innovations\n",
        "- Self Attention Mechanism: introduced the scaled dot product attention mechanism to capture relationships between all words in a sequence, regardless of distance. This mechanism computes attention weights for all pair of words, enabling parallelization.\n",
        "\n",
        "- Multi head Attention: Extended seld attention by using multiple attention heads, allowing the model to learn different aspects of the input data simultaneously.\n",
        "\n",
        "- Positional Encoding: Added positional information to the input embeddings, allowing the mode to handle sequences without recurrence or convolution.\n",
        "\n",
        "- Fully Feed Forward Architecture: Removed RNNs and convolution, replacing them with attention mechanism and feed forward networks, resulting in better parallelization and faster training\n",
        "\n",
        "- Encoder Decode Architecture: The transformer uses separate encoder and decoder stacks for tasks like machine translation and text generation.\n",
        "\n",
        "- Scalability: Showed excellent scalability with respect to dataset size, making it ideal for large scaled natural language processing\n"
      ],
      "metadata": {
        "id": "lC8cjnUoioPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math"
      ],
      "metadata": {
        "id": "uKHa5hs4IR3q"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scaled dot product\n",
        "This mechanism computes the relevance of each token in a sequence with every other token using the dot product between the query and key vectors.\n",
        "\n",
        "The result is scaled down by squareroot of d_k to prevent large values causing gradients to vanish or explode."
      ],
      "metadata": {
        "id": "wpDEJZ1HKIJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaled Dot product attention\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self,d_k):\n",
        "      super(ScaledDotProductAttention, self).__init__()\n",
        "      self.d_k = d_k\n",
        "\n",
        "    def forward(self, query, key, value, mask = None):\n",
        "      scores = torch.matmul(query, key.transpose(-2,-1)) / math.sqrt(self.d_k)\n",
        "      if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "      attention = torch.softmax(scores, dim = -1)\n",
        "      context = torch.matmul(attention, value)\n",
        "      return context, attention"
      ],
      "metadata": {
        "id": "_S2up8q4IaUw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi Head Attention\n",
        "Extends the single attentnion mechanism by splitting the input into multiple heads.\n",
        "\n",
        "Each head independently computes attentio, allowing the model to focus on different aspects of the data simultaneously."
      ],
      "metadata": {
        "id": "iJqa_MgQKuul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "    self.d_k = d_model // num_heads\n",
        "    self.num_heads = num_heads\n",
        "\n",
        "    self.query = nn.Linear(d_model, d_model)\n",
        "    self.key = nn.Linear(d_model, d_model)\n",
        "    self.value = nn.Linear(d_model, d_model)\n",
        "    self.fc = nn.Linear(d_model, d_model)\n",
        "\n",
        "  def forward(self, query, key, value, mask = None):\n",
        "    batch_size = query.size(0)\n",
        "    query = self.query(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2)\n",
        "    key = self.key(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2)\n",
        "    value = self.value(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2)\n",
        "\n",
        "    output, attention = ScaledDotProductAttention(self.d_k)(query, key, value, mask)\n",
        "    output = output.transpose(1,2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n",
        "    return self.fc(output)"
      ],
      "metadata": {
        "id": "aq6UebFJI0U9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Positional Encoding\n",
        "Since transformers do not use recurrence or convoluion, they lack the natural order of sequences. Positional encoding explicitly adds positional information to embeddings using sine and cosine fucntions."
      ],
      "metadata": {
        "id": "sxL8hFqjK_lV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model, max_len = 5000):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "    self.encoding = torch.zeros(max_len, d_model)\n",
        "    position = torch.arange(0, max_len).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
        "    self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
        "    self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
        "    self.encoding = self.encoding.unsqueeze(0)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x + self.encoding[:, :x.size(1)].to(x.device)"
      ],
      "metadata": {
        "id": "0s8F73YmJg6z"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        attention_output = self.attention(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attention_output))\n",
        "        ff_output = self.ff(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "KxiSRYG8J2IF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, d_model, num_heads, d_ff, num_layers, max_len=100):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerBlock(d_model, num_heads, d_ff) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.fc = nn.Linear(d_model, output_dim)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = self.embedding(x)\n",
        "        x = self.positional_encoding(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.fc(x)"
      ],
      "metadata": {
        "id": "N-PnU2_fJ6MU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_transformer():\n",
        "    # Hyperparameters\n",
        "    input_dim = 5000  # Vocabulary size\n",
        "    output_dim = 5000\n",
        "    d_model = 128\n",
        "    num_heads = 8\n",
        "    d_ff = 512\n",
        "    num_layers = 4\n",
        "    max_len = 100\n",
        "    batch_size = 64\n",
        "    epochs = 10\n",
        "\n",
        "    # Synthetic Dataset (English to French translation)\n",
        "    train_data = torch.randint(0, input_dim, (1000, max_len))\n",
        "    train_labels = torch.randint(0, output_dim, (1000, max_len))\n",
        "\n",
        "    # DataLoader\n",
        "    train_loader = torch.utils.data.DataLoader(list(zip(train_data, train_labels)), batch_size=batch_size)\n",
        "\n",
        "    # Model\n",
        "    model = Transformer(input_dim, output_dim, d_model, num_heads, d_ff, num_layers).to('cuda')\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to('cuda'), y.to('cuda')\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(x)\n",
        "            loss = criterion(outputs.view(-1, output_dim), y.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# Train Transformer\n",
        "train_transformer()"
      ],
      "metadata": {
        "id": "Ih9pC_nxJ_4i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}